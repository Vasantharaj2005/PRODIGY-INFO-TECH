{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "from skimage import io\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# # Starts capturing video\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     cv2.imshow('Captured Frame', frame)\n",
    "#     if cv2.waitKey(1) == ord('q'):\n",
    "#         break\n",
    "\n",
    "#     keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "# To segment the region of hand in the image\n",
    "#---------------------------------------------\n",
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 118, 32)       320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 98, 118, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 59, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 49, 59, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 47, 57, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 47, 57, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 23, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 41216)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               5275776   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 5,295,750\n",
      "Trainable params: 5,295,558\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# model\n",
    "model = Sequential()\n",
    "\n",
    "# first conv layer\n",
    "# input shape = (img_rows, img_cols, 1)\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100,120, 1))) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# second conv layer\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu')) # fully connected\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam() \n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/hand-gesture-recog-dataset/data/five',\n",
       " '/kaggle/input/hand-gesture-recog-dataset/data/thumbsdown',\n",
       " '/kaggle/input/hand-gesture-recog-dataset/data/fist',\n",
       " '/kaggle/input/hand-gesture-recog-dataset/data/blank',\n",
       " '/kaggle/input/hand-gesture-recog-dataset/data/ok',\n",
       " '/kaggle/input/hand-gesture-recog-dataset/data/thumbsup']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "DATASET_PATH = '/kaggle/input/hand-gesture-recog-dataset/data'\n",
    "\n",
    "dataset_path = os.path.join(DATASET_PATH, '*')\n",
    "import glob\n",
    "dataset_path = glob.glob(dataset_path)\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f68f4071470>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "image = cv2.imread('/kaggle/input/hand-gesture-recog-dataset/data/five/hand1(1015).jpg')\n",
    "image = cv2.resize(image,(100, 120))\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600\n",
      "9600\n"
     ]
    }
   ],
   "source": [
    "loaded_images = []\n",
    "\n",
    "list_of_gestures = ['blank', 'ok', 'thumbsup', 'thumbsdown', 'fist', 'five']\n",
    "\n",
    "for path in range(0, len(dataset_path)):\n",
    "    dataset_path = \"/kaggle/input/hand-gesture-recog-dataset/data/\" + str(list_of_gestures[path])\n",
    "    gesture_path = os.path.join(dataset_path, '*')\n",
    "    import glob\n",
    "    gest_path = glob.glob(gesture_path)\n",
    "    k = 0\n",
    "    for i in range(0, len(gest_path)):\n",
    "        if k < 1600:\n",
    "            image = cv2.imread(gest_path[i])\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            gray_image = cv2.resize(gray_image,(100, 120))\n",
    "            loaded_images.append(gray_image)\n",
    "        k=k+1\n",
    "print(len(loaded_images))\n",
    "\n",
    "outputVectors = []\n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([1, 0, 0, 0, 0, 0])\n",
    "\n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 1, 0, 0, 0, 0])\n",
    "\n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 0, 1, 0, 0, 0])\n",
    "    \n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 0, 0, 1, 0, 0])\n",
    "    \n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 0, 0, 0, 1, 0])\n",
    "\n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 0, 0, 0, 0, 1])\n",
    "\n",
    "print(len(outputVectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9600, 120, 100)\n",
      "(9600, 6)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(loaded_images)\n",
    "y = np.asarray(outputVectors)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7680, 100, 120, 1)\n",
      "(1920, 100, 120, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "X_train = X_train.reshape(X_train.shape[0], 100, 120, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 100, 120, 1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7680 samples, validate on 1920 samples\n",
      "Epoch 1/10\n",
      "7680/7680 [==============================] - 9s 1ms/step - loss: 2.8744 - categorical_accuracy: 0.7292 - val_loss: 0.7971 - val_categorical_accuracy: 0.7724\n",
      "Epoch 2/10\n",
      "7680/7680 [==============================] - 4s 570us/step - loss: 0.3010 - categorical_accuracy: 0.8936 - val_loss: 0.0491 - val_categorical_accuracy: 0.9812\n",
      "Epoch 3/10\n",
      "7680/7680 [==============================] - 4s 577us/step - loss: 0.2151 - categorical_accuracy: 0.9254 - val_loss: 0.0545 - val_categorical_accuracy: 0.9792\n",
      "Epoch 4/10\n",
      "7680/7680 [==============================] - 4s 570us/step - loss: 0.1890 - categorical_accuracy: 0.9348 - val_loss: 0.0341 - val_categorical_accuracy: 0.9870\n",
      "Epoch 5/10\n",
      "7680/7680 [==============================] - 4s 573us/step - loss: 0.1594 - categorical_accuracy: 0.9380 - val_loss: 0.1052 - val_categorical_accuracy: 0.9536\n",
      "Epoch 6/10\n",
      "7680/7680 [==============================] - 4s 572us/step - loss: 0.1288 - categorical_accuracy: 0.9430 - val_loss: 0.0508 - val_categorical_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "7680/7680 [==============================] - 4s 583us/step - loss: 0.1274 - categorical_accuracy: 0.9495 - val_loss: 0.0304 - val_categorical_accuracy: 0.9875\n",
      "Epoch 8/10\n",
      "7680/7680 [==============================] - 4s 578us/step - loss: 0.1068 - categorical_accuracy: 0.9577 - val_loss: 0.0203 - val_categorical_accuracy: 0.9911\n",
      "Epoch 9/10\n",
      "7680/7680 [==============================] - 4s 579us/step - loss: 0.0922 - categorical_accuracy: 0.9624 - val_loss: 0.0063 - val_categorical_accuracy: 0.9974\n",
      "Epoch 10/10\n",
      "7680/7680 [==============================] - 4s 573us/step - loss: 0.0859 - categorical_accuracy: 0.9655 - val_loss: 0.0122 - val_categorical_accuracy: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f64b264cf98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model with data\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "# model.save(\"hand_gesture_recognition.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 0s 238us/step\n",
      "Accuracy: 0.9927083253860474\n"
     ]
    }
   ],
   "source": [
    "[loss, acc] = model.evaluate(X_test,y_test,verbose=1)\n",
    "print(\"Accuracy: \" + str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
